{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## このプログラムは「Mercari Price Suggestion Challenge」において\n",
    "## 作成したノートブックで動作します\n",
    "\n",
    "%%time\n",
    "from datetime import datetime \n",
    "start_real = datetime.now()\n",
    "\n",
    "import pandas as pd\n",
    "# 訓練データテストデータをデータフレームに読み込む\n",
    "train_df = pd.read_table('../input/mercari/train.tsv')\n",
    "test_df = pd.read_table('../input/mercari/test.tsv')\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ドル未満のレコードをすべて削除する\n",
    "train_df = train_df.drop(train_df[(train_df.price < 3.0)].index)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 商品名と商品説明の単語の数を調べる\n",
    "\n",
    "def wordCount(text):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      text(str): 商品名、商品の説明文\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if text == 'No description yet':\n",
    "            return 0  # 商品名や説明が'No description yet'の場合は0を返す\n",
    "        else:\n",
    "            text = text.lower()                  # すべて小文字にする\n",
    "            words = [w for w in text.split(\" \")] # スペースで切り分ける\n",
    "            return len(words)                    # 単語の数を返す\n",
    "    except: \n",
    "        return 0\n",
    "\n",
    "# 'name'の各フィールドの単語数を'name_len'に登録\n",
    "train_df['name_len'] = train_df['name'].apply(lambda x: wordCount(x))\n",
    "test_df['name_len'] = test_df['name'].apply(lambda x: wordCount(x))\n",
    "# 'item_description'の各フィールドの単語数を'desc_len'に登録\n",
    "train_df['desc_len'] = train_df['item_description'].apply(lambda x: wordCount(x))\n",
    "test_df['desc_len'] = test_df['item_description'].apply(lambda x: wordCount(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 訓練データのpriceのスケーリング\n",
    "import numpy as np\n",
    "\n",
    "# 訓練データの'price'を対数変換する\n",
    "train_df[\"target\"] = np.log1p(train_df.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# カテゴリ名を切り分けて新設のカラムに登録する\n",
    "\n",
    "def split_cat(text):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      text(str): カテゴリ名\n",
    "\n",
    "    ・カテゴリを/で切り分ける\n",
    "    ・データが存在しない場合は\"No Label\"を返す\n",
    "    \"\"\"\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "# 3つに切り分けたカテゴリ名を'subcat_0'、'subcat_1'、'subcat_2'に登録\n",
    "# 訓練データ\n",
    "train_df['subcat_0'], train_df['subcat_1'], train_df['subcat_2'] = \\\n",
    "    zip(*train_df['category_name'].apply(lambda x: split_cat(x)))\n",
    "# テストデータ\n",
    "test_df['subcat_0'], test_df['subcat_1'], test_df['subcat_2'] = \\\n",
    "    zip(*test_df['category_name'].apply(lambda x: split_cat(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 'brand_name'の対策\n",
    "\n",
    "# train_dfとtest_dfを縦方向に結合\n",
    "full_set = pd.concat([train_df, test_df])\n",
    "# full_setの'brand_name'から重複なしのブランドリスト(集合)を生成\n",
    "all_brands = set(full_set['brand_name'].values)\n",
    "\n",
    "# 'brand_name'の欠損値NaNを'missing'に置き換える\n",
    "train_df['brand_name'].fillna(value='missing', inplace=True)\n",
    "test_df['brand_name'].fillna(value='missing', inplace=True)\n",
    "\n",
    "# 訓練データの'brand_name'が'missing'に一致するレコード数を取得\n",
    "train_premissing = len(train_df.loc[train_df['brand_name'] == 'missing'])\n",
    "# テストデータの'brand_name'が'missing'に一致するレコード数を取得\n",
    "test_premissing = len(test_df.loc[test_df['brand_name'] == 'missing'])\n",
    "\n",
    "def brandfinder(line):\n",
    "    \"\"\"\n",
    "    Parameters: line(str): ブランド名\n",
    "\n",
    "    ・ブランド名の'missing'を商品名に置き換える:\n",
    "         missing'の商品名の単語がブランドリストに存在する場合\n",
    "    ・ブランド名を商品名に置き換える:\n",
    "        商品名がブランドリストの名前と完全に一致する場合\n",
    "    ・ブランド名をそのままにする:\n",
    "        商品名がブランドリストの名前と一致しない\n",
    "        ブランド名が'missing'だが商品名の単語がブランドリストにない\n",
    "    \"\"\"\n",
    "    brand = line[0] # 第1要素はブランド名\n",
    "    name = line[1]  # 第2要素は商品名\n",
    "    namesplit = name.split(' ') # 商品名をスペースで切り分ける\n",
    "    \n",
    "    if brand == 'missing':  # ブランド名が'missing'と一致\n",
    "        for x in namesplit: # 商品名から切り分けた単語を取り出す\n",
    "            if x in all_brands:                \n",
    "                return name # 単語がブランドリストに一致したら商品名を返す\n",
    "    if name in all_brands:  # 商品名がブランドリストに存在すれば商品名を返す\n",
    "        return name\n",
    "    \n",
    "    return brand            # どれにも一致しなければブランド名を返す\n",
    "\n",
    "# ブランド名の付替えを実施\n",
    "train_df['brand_name'] = train_df[['brand_name','name']].apply(brandfinder, axis = 1)\n",
    "test_df['brand_name'] = test_df[['brand_name','name']].apply(brandfinder, axis = 1)\n",
    "\n",
    "# 書き換えられた'missing'の数を取得\n",
    "train_found = train_premissing-len(train_df.loc[train_df['brand_name'] == 'missing'])\n",
    "test_found = test_premissing-len(test_df.loc[test_df['brand_name'] == 'missing'])\n",
    "print(train_premissing) # 書き換える前の'missing'の数\n",
    "print(train_found)      # 書き換えられた'missing'の数\n",
    "print(test_premissing)  # 書き換える前の'missing'の数\n",
    "print(test_found)       # 書き換えられた'missing'の数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 訓練用のデータフレームを訓練用と検証用に99:1で分割する\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "train_dfs, dev_dfs = train_test_split(\n",
    "    train_df,         # 対象のデータフレーム\n",
    "    random_state=123, # 乱数生成時のシード(種)\n",
    "    train_size=0.99,  # 訓練用に99%のデータ\n",
    "    test_size=0.01)   # 検証用に1%のデータ\n",
    "\n",
    "n_trains = train_dfs.shape[0] # 訓練データのサイズ\n",
    "n_devs = dev_dfs.shape[0]     # 検証データのサイズ\n",
    "n_tests = test_df.shape[0]   # テストデータのサイズ\n",
    "print('Training :', n_trains, 'examples')\n",
    "print('Validating :', n_devs, 'examples')\n",
    "print('Testing :', n_tests, 'examples')\n",
    "\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 訓練データ、検証データ、テストデータを1つのデータフレームに連結\n",
    "full_df = pd.concat([train_dfs, dev_dfs, test_df])\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    \"\"\"連結データのカテゴリ名、ブランド名、説明文のNaNを'missing'に置き換える\n",
    "    \n",
    "    Parameter:\n",
    "        df: すべてのデータを連結したデータフレーム\n",
    "    \"\"\"\n",
    "    df.category_name.fillna(value='missing', inplace=True)    # カテゴリ名\n",
    "    df.brand_name.fillna(value='missing', inplace=True)       # ブランド名\n",
    "    df.item_description.fillna(value='missing', inplace=True) # 説明文\n",
    "    # 説明文の'No description yet'を'missing'にする\n",
    "    df.item_description.replace(\n",
    "        'No description yet','missing', inplace=True) # 説明文の置き換え\n",
    "    return df\n",
    "\n",
    "full_df = fill_missing_values(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# カテゴリ、ブランド、3カテゴリのテキストをラベルエンコードする\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Processing categorical data...\")\n",
    "\n",
    "# LabelEncoderの生成\n",
    "le = LabelEncoder()\n",
    "\n",
    "# 'category_name'をエンコードして'category'カラムに登録する\n",
    "le.fit(full_df.category_name)\n",
    "full_df['category'] = le.transform(full_df.category_name)\n",
    "\n",
    "# 'brand_name'のエンコード\n",
    "le.fit(full_df.brand_name)\n",
    "full_df.brand_name = le.transform(full_df.brand_name)\n",
    "\n",
    "# 'subcat_0'のエンコード\n",
    "le.fit(full_df.subcat_0)\n",
    "full_df.subcat_0 = le.transform(full_df.subcat_0)\n",
    "\n",
    "# 'subcat_1'のエンコード\n",
    "le.fit(full_df.subcat_1)\n",
    "full_df.subcat_1 = le.transform(full_df.subcat_1)\n",
    "\n",
    "# 'subcat_2'のエンコード\n",
    "le.fit(full_df.subcat_2)\n",
    "full_df.subcat_2 = le.transform(full_df.subcat_2)\n",
    "\n",
    "del le\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 連結データの説明文、商品名をTokenizerでラベルエンコードする\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 説明文、商品名、カテゴリ名の配列要素を以下のように1次元配列に連結する\n",
    "# [[説明1,説明2, ..., 商品1,商品2, ..., カテゴリ1,カテゴリ2, ...]\n",
    "#\n",
    "print(\"Transforming text data to sequences...\")\n",
    "raw_text = np.hstack(\n",
    "    [full_df.item_description.str.lower(), # 説明文\n",
    "     full_df.name.str.lower(),             # 商品名\n",
    "     full_df.category_name.str.lower()]    # カテゴリ名\n",
    ")\n",
    "print('sequences shape', raw_text.shape)\n",
    "\n",
    "print(\"   Fitting tokenizer...\")\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "\n",
    "print(\"   Transforming text to sequences...\")\n",
    "full_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df.item_description.str.lower())\n",
    "full_df['seq_name'] = tok_raw.texts_to_sequences(full_df.name.str.lower())\n",
    "\n",
    "del tok_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNモデルで使用する定数を定義\n",
    "\n",
    "MAX_NAME_SEQ = 10      # 商品名の最大サイズ(最大17を10に切り詰める)\n",
    "MAX_ITEM_DESC_SEQ = 75 # 商品説明の最大サイズ(最大269を75に切り詰める)\n",
    "MAX_CATEGORY_SEQ = 8   # カテゴリ名の最大サイズ(最大8)\n",
    "\n",
    "# 商品名ラベルと商品説明の単語数: ラベルの最大値+100\n",
    "MAX_TEXT = np.max([\n",
    "    np.max(full_df.seq_name.max()),\n",
    "    np.max(full_df.seq_item_description.max())\n",
    "]) + 100\n",
    "# カテゴリ名の単語数: カテゴリラベルの最大値+1\n",
    "MAX_CATEGORY = np.max(full_df.category.max()) + 1\n",
    "# ブランド名の単語数: ブランドラベルの最大値+1\n",
    "MAX_BRAND = np.max(full_df.brand_name.max()) + 1\n",
    "# 商品状態の数: 商品状態の最大値+1\n",
    "MAX_CONDITION = np.max(full_df.item_condition_id.max()) + 1\n",
    "# 商品説明の単語数: レコードごとの単語数の最大値+1\n",
    "MAX_DESC_LEN = np.max(full_df.desc_len.max()) + 1\n",
    "# 商品名の単語数: レコードごとの単語数の最大値+1\n",
    "MAX_NAME_LEN = np.max(full_df.name_len.max()) + 1\n",
    "# サブカテゴリの単語数: 各ラベルの最大値+1\n",
    "MAX_SUBCAT_0 = np.max(full_df.subcat_0.max()) + 1\n",
    "MAX_SUBCAT_1 = np.max(full_df.subcat_1.max()) + 1\n",
    "MAX_SUBCAT_2 = np.max(full_df.subcat_2.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# RNNに入力するデータを用意する\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_rnn_data(dataset):\n",
    "    \"\"\"\n",
    "    入力データをdictオブジェクトに格納して返す\n",
    "    \n",
    "    Parameter: \n",
    "      dataset: フル結合のデータフレーム\n",
    "    \"\"\"\n",
    "    X = {\n",
    "        # 商品名ラベル(ndarray(int))\n",
    "        # 0パディングして配列のサイズを統一: MAX_NAME_SEQ=10\n",
    "        'name': pad_sequences(dataset.seq_name,\n",
    "                              maxlen=MAX_NAME_SEQ),\n",
    "        # 商品説明ラベル(ndarray(int))\n",
    "        # 0パディングして配列のサイズを統一: MAX_ITEM_DESC_SEQ=75\n",
    "        'item_desc': pad_sequences(dataset.seq_item_description,\n",
    "                                   maxlen=MAX_ITEM_DESC_SEQ),\n",
    "        # ブランド名のラベル(ndarray(int))\n",
    "        'brand_name': np.array(dataset.brand_name),\n",
    "        # /区切りのカテゴリ名のラベル(ndarray(int))\n",
    "        'category': np.array(dataset.category),\n",
    "        # 商品の状態(ndarray(int))\n",
    "        'item_condition': np.array(dataset.item_condition_id),\n",
    "        # 送料負担(ndarray(int)):出品者負担は1,購入者負担は0\n",
    "        'num_vars': np.array(dataset[[\"shipping\"]]),\n",
    "        # 商品説明の単語数(ndarray(int))\n",
    "        'desc_len': np.array(dataset[[\"desc_len\"]]),\n",
    "        # 商品名の単語数(ndarray(int))\n",
    "        'name_len': np.array(dataset[[\"name_len\"]]),\n",
    "        # カテゴリ0のラベル(ndarray(int))\n",
    "        'subcat_0': np.array(dataset.subcat_0),\n",
    "        # カテゴリ1のラベル(ndarray(int))\n",
    "        'subcat_1': np.array(dataset.subcat_1),\n",
    "        # カテゴリ2のラベル(ndarray(int))\n",
    "        'subcat_2': np.array(dataset.subcat_2),\n",
    "    }\n",
    "    return X\n",
    "\n",
    "# フル結合のデータフレームから抽出\n",
    "# 訓練データ: インデックス0～訓練データ数-1のインデックスまで\n",
    "train = full_df[:n_trains]\n",
    "# 検証データ: 訓練データ数～訓練データ数+検証データ数-1のインデックスまで\n",
    "dev = full_df[n_trains:n_trains+n_devs]\n",
    "# テストデータ: 訓練データ+検証データから末尾まで\n",
    "test = full_df[n_trains+n_devs:]\n",
    "\n",
    "# 訓練用のdictを取得\n",
    "X_train = get_rnn_data(train)\n",
    "# 訓練用の商品価格の1階テンソルを2階テンソルに変換\n",
    "# (14817,)→(14817,1)\n",
    "Y_train = train.target.values.reshape(-1, 1)\n",
    "\n",
    "# 検証用のdictを取得\n",
    "X_dev = get_rnn_data(dev)\n",
    "# 検証用の商品価格の1階テンソルを2階テンソルい変換\n",
    "# (146844,)→(146844,1)\n",
    "Y_dev = dev.target.values.reshape(-1, 1)\n",
    "\n",
    "# テスト用のdictを取得\n",
    "X_test = get_rnn_data(test)\n",
    "\n",
    "del full_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Embedding, Flatten\n",
    "from tensorflow.keras.layers import concatenate, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "np.random.seed(123) # 乱数のシードを設定\n",
    "\n",
    "# RMSE(Root Mean Square Error):二乗平均平方根誤差\n",
    "#\n",
    "# 予測をチェックするために使用\n",
    "# この関数を使用する際のYとY_predはすでにlogスケールになっているので、RMSLE(対数二乗平均平方根誤差)のように機能する\n",
    "def rmsle(Y, Y_pred):\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    return np.sqrt(np.mean(np.square(Y_pred - Y )))\n",
    "\n",
    "def new_rnn_model(lr=0.001, decay=0.0):\n",
    "    \"\"\"RNNのモデルを生成\n",
    "    \n",
    "    Parameters:\n",
    "      lr: 学習率\n",
    "      decay: 学習率減衰\n",
    "    \"\"\"\n",
    "    # 入力層\n",
    "    # 商品名ラベル、商品説明ラベル、ブランド名ラベル、商品の状態、送料負担、\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    # 商品名テキスト、商品説明テキストの単語数\n",
    "    name_len = Input(shape=[1], name=\"name_len\")\n",
    "    desc_len = Input(shape=[1], name=\"desc_len\")\n",
    "    # サブカテゴリラベル0～2\n",
    "    subcat_0 = Input(shape=[1], name=\"subcat_0\")\n",
    "    subcat_1 = Input(shape=[1], name=\"subcat_1\")\n",
    "    subcat_2 = Input(shape=[1], name=\"subcat_2\")\n",
    "\n",
    "    # Embedding層\n",
    "    # 商品名のEmbedding: 入力は単語の総数+100、出力の次元数は20\n",
    "    emb_name = Embedding(MAX_TEXT, 20)(name)\n",
    "    # 商品説明のEmbedding: 入力は単語の総数+100、出力の次元数は60\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 60)(item_desc)\n",
    "    # ブランド名のEmbedding: 入力は単語の総数+1、出力の次元数は10\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "    # 商品状態のEmbedding: 入力は5+1、出力の次元数は5\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    # 商品説明の単語数のEmbedding: 入力は商品説明の最大単語数+1、出力は5\n",
    "    emb_desc_len = Embedding(MAX_DESC_LEN, 5)(desc_len)\n",
    "    # 商品名の単語数のEmbedding: 入力は商品名の最大単語数+1、出力は5\n",
    "    emb_name_len = Embedding(MAX_NAME_LEN, 5)(name_len)\n",
    "    # サブカテゴリ0～2のEmbedding: 入力はカテゴリ名の最大単語数+1、出力は10\n",
    "    emb_subcat_0 = Embedding(MAX_SUBCAT_0, 10)(subcat_0)\n",
    "    emb_subcat_1 = Embedding(MAX_SUBCAT_1, 10)(subcat_1)\n",
    "    emb_subcat_2 = Embedding(MAX_SUBCAT_2, 10)(subcat_2)\n",
    "    \n",
    "    # ReccuurentユニットはLSTMより高速なGRU\n",
    "    rnn_layer1 = GRU(16) (emb_item_desc) # 商品説明のユニット\n",
    "    rnn_layer2 = GRU(8) (emb_name)       # 商品名のユニット\n",
    "\n",
    "    # 全結合層\n",
    "    main_l = concatenate([\n",
    "        Flatten()(emb_brand_name),     # ブランド名のEmbedding\n",
    "        Flatten()(emb_item_condition), # 商品状態のEmbedding\n",
    "        Flatten()(emb_desc_len), # 商品説明の単語数のEmbedding\n",
    "        Flatten()(emb_name_len), # 商品名の単語数のEmbedding\n",
    "        Flatten()(emb_subcat_0), # サブカテゴリ0のEmbedding\n",
    "        Flatten()(emb_subcat_1), # サブカテゴリ1のEmbedding\n",
    "        Flatten()(emb_subcat_2), # サブカテゴリ2のEmbedding\n",
    "        rnn_layer1, # 商品説明のGRUユニット\n",
    "        rnn_layer2, # 商品名のGRUユニット\n",
    "        num_vars    # 送料負担(0か1)\n",
    "    ])\n",
    "    # 512、256、128、64ユニットの層を追加\n",
    "    main_l = Dropout(0.1)(\n",
    "        Dense(512,kernel_initializer='normal',activation='relu')(main_l))\n",
    "    main_l = Dropout(0.1)(\n",
    "        Dense(256,kernel_initializer='normal',activation='relu')(main_l))\n",
    "    main_l = Dropout(0.1)(\n",
    "        Dense(128,kernel_initializer='normal',activation='relu')(main_l))\n",
    "    main_l = Dropout(0.1)(\n",
    "        Dense(64,kernel_initializer='normal',activation='relu')(main_l))\n",
    "\n",
    "    # 出力層(1ユニット)\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "    \n",
    "    # Modelオブジェクトの生成\n",
    "    model = Model(\n",
    "        # 入力層はマルチ入力モデルなのでリストにする\n",
    "        inputs=[name, item_desc, brand_name, item_condition, num_vars,\n",
    "                desc_len, name_len, subcat_0, subcat_1, subcat_2],\n",
    "        # 出力層\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    # 平均2乗誤差関数とオプティマイザーをセットしてコンパイル\n",
    "    model.compile(loss = 'mse',\n",
    "                  optimizer = Adam(lr=lr, decay=decay))\n",
    "\n",
    "    return model\n",
    "\n",
    "# RNNのモデルを生成\n",
    "model = new_rnn_model()\n",
    "model.summary()\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# ミニバッチのサイズ\n",
    "BATCH_SIZE = 512 * 2\n",
    "epochs = 3\n",
    "\n",
    "# 学習率減衰\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "steps = int(len(X_train['name']) / BATCH_SIZE) * epochs\n",
    "lr_init =  0.005\n",
    "lr_fin = 0.001\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "\n",
    "# モデルを生成\n",
    "rnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\n",
    "# 学習\n",
    "rnn_model.fit(X_train, Y_train,\n",
    "              epochs=epochs,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              validation_data=(X_dev, Y_dev),\n",
    "              verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 検証データでモデルを評価する\n",
    "\n",
    "print(\"Evaluating the model on validation data...\")\n",
    "# 学習済みのモデルで検証データの予測を行う\n",
    "Y_dev_preds_rnn = rnn_model.predict(X_dev,\n",
    "                                    batch_size=BATCH_SIZE)\n",
    "# 予測値の損失をrmsle()で求める\n",
    "print(\" RMSLE error:\", rmsle(Y_dev, # 検証データの商品価格\n",
    "                             Y_dev_preds_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータで予測する\n",
    "rnn_preds = rnn_model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "# 指数関数\n",
    "rnn_preds = np.expm1(rnn_preds)\n",
    "\n",
    "del rnn_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridgeモデルのための前処理\n",
    "\n",
    "# 訓練データ、検証データ、テストデータを結合する\n",
    "full_df2 = pd.concat([train_dfs, dev_dfs, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 欠損値を処理し、すべてのデータを文字列にする\n",
    "print(\"Handling missing values...\")\n",
    "# カテゴリ名の欠損値を'missing'に置き換える\n",
    "full_df2['category_name'] = \\\n",
    "    full_df2['category_name'].fillna('missing').astype(str)\n",
    "# サブカテゴリのラベルを文字列に変換\n",
    "full_df2['subcat_0'] = full_df2['subcat_0'].astype(str)\n",
    "full_df2['subcat_1'] = full_df2['subcat_1'].astype(str)\n",
    "full_df2['subcat_2'] = full_df2['subcat_2'].astype(str)\n",
    "# ブランド名の欠損値を'missing'に置き換える\n",
    "full_df2['brand_name'] = full_df2['brand_name'].fillna('missing').astype(str)\n",
    "# 送料負担、商品の状態を文字列に置き換える\n",
    "full_df2['shipping'] = full_df2['shipping'].astype(str)\n",
    "full_df2['item_condition_id'] = full_df2['item_condition_id'].astype(str)\n",
    "# 説明文の単語数、商品名の単語数を文字列に置き換える\n",
    "full_df2['desc_len'] = full_df2['desc_len'].astype(str)\n",
    "full_df2['name_len'] = full_df2['name_len'].astype(str)\n",
    "# 説明文の欠損値を'No description yet'に置き換える\n",
    "full_df2['item_description'] = \\\n",
    "    full_df2['item_description'].fillna('No description yet').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# すべてのデータをBag of Wordsでベクトル化する\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "print(\"Vectorizing data...\")\n",
    "# CountVectorizerの生成処理を関数化\n",
    "default_preprocessor = CountVectorizer().build_preprocessor()\n",
    "\n",
    "def build_preprocessor(field):\n",
    "    \"\"\"\n",
    "    指定されたカラムのインデックスを取得し、\n",
    "    トークンカウント行列を作成するためのCountVectorizerを返す\n",
    "    \n",
    "    Parameter:field(str)\n",
    "      フル結合データフレームのカラム名\n",
    "    \"\"\"\n",
    "    field_idx = list(full_df2.columns).index(field)\n",
    "    return lambda x: default_preprocessor(x[field_idx])\n",
    "\n",
    "# トークンカウント行列\n",
    "# CountVectorizeを結合して\n",
    "# (識別子, ベクトライザーオブジェクト)のリストで構成される\n",
    "# トランスファーマーオブジェクトを生成\n",
    "vectorizer = FeatureUnion([\n",
    "    ('name', CountVectorizer(\n",
    "        # bag-of-wordで分割する単位をn-gramで連続する単語のつながりとする\n",
    "        # 商品名は2-gram(2つの単語のつながり)で分割\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=5000, # トークンカウントの上限値############### 50000\n",
    "        # トークンカウントステップをオーバーライド\n",
    "        preprocessor=build_preprocessor('name'))),\n",
    "    ('subcat_0', CountVectorizer(\n",
    "        # トークンの構成を示す正規表現を'+'として1文字に対応\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('subcat_0'))),\n",
    "    ('subcat_1', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('subcat_1'))),\n",
    "    ('subcat_2', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('subcat_2'))),\n",
    "    ('brand_name', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('brand_name'))),\n",
    "    ('shipping', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('shipping'))),\n",
    "    ('item_condition_id', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('item_condition_id'))),\n",
    "    ('desc_len', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('desc_len'))),\n",
    "    ('name_len', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('name_len'))),\n",
    "    ('item_description', TfidfVectorizer(\n",
    "        # bag-of-wordで分割する単位をn-gramで連続する単語のつながりとする\n",
    "        # 商品説名は3-gram(3つの単語のつながり)で分割\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=5000, # トークンカウントの上限値#################### 100000\n",
    "        preprocessor=build_preprocessor('item_description'))),\n",
    "])\n",
    "\n",
    "# フル結合のデータフレームのフィールド値を\n",
    "# n-grainによるbag-of-wordsでトークンカウント行列に変換する\n",
    "X = vectorizer.fit_transform(full_df2.values)\n",
    "\n",
    "del vectorizer\n",
    "gc.collect()\n",
    "\n",
    "# 入力用のdictオブジェクトから訓練用のデータを抽出\n",
    "X_train = X[:n_trains]\n",
    "# 訓練用のデータフレームから商品価格を抽出して\n",
    "# (データ数, 価格)の2階テンソルに変換\n",
    "Y_train = train_dfs.target.values.reshape(-1, 1)\n",
    "\n",
    "# 入力用のdictオブジェクトから検証用のデータを抽出\n",
    "X_dev = X[n_trains:n_trains+n_devs]\n",
    "# 検証用のデータフレームから商品価格を抽出して\n",
    "# (データ数, 価格)の2階テンソルに変換\n",
    "Y_dev = dev_dfs.target.values.reshape(-1, 1)\n",
    "\n",
    "# 入力用のdictオブジェクトからテストデータを抽出\n",
    "X_test = X[n_trains+n_devs:]\n",
    "\n",
    "print('X:', X.shape)\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_dev:', X_dev.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('Y_train:', Y_train.shape)\n",
    "print('Y_dev:', Y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Ridge、RidgeCVで最適化する\n",
    "\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "print(\"Fitting Ridge model on training examples...\")\n",
    "ridge_model = Ridge(\n",
    "    solver='auto',      # ソルバーをオートモードにする\n",
    "    fit_intercept=True, # 切片を計算に使用\n",
    "    alpha=1.0,          # 正則化の強度はデフォルト値\n",
    "    max_iter=200,       # ソルバーの最大反復回数\n",
    "    normalize=False,    # 正規化を行う\n",
    "    tol=0.01,           # 回帰の反復を停止するときの精度\n",
    "    # データをシャッフルするときに使用する疑似乱数ジェネレータのシード\n",
    "    random_state = 1,\n",
    ")\n",
    "\n",
    "print(\"Fitting RidgeCV model on training examples...\")\n",
    "ridge_modelCV = RidgeCV(\n",
    "    fit_intercept=True, # 切片を計算に使用\n",
    "    alphas=[5.0],\n",
    "    normalize=False,\n",
    "    cv = 2, # 交差検証時にスコアを2回連続して(毎回異なる分割で)計算\n",
    "    # モデルの評価は平均2乗誤差回帰損失で行う\n",
    "    scoring='neg_mean_squared_error',\n",
    ")\n",
    "\n",
    "ridge_model.fit(X_train, Y_train)\n",
    "ridge_modelCV.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridgeモデルに検証データを入力して損失を測定\n",
    "Y_dev_preds_ridge = ridge_model.predict(X_dev)\n",
    "Y_dev_preds_ridge = Y_dev_preds_ridge.reshape(-1, 1)\n",
    "print('Ridge model RMSL error:', rmsle(Y_dev, Y_dev_preds_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeCVモデルに検証データを入力して損失を測定\n",
    "Y_dev_preds_ridgeCV = ridge_modelCV.predict(X_dev)\n",
    "Y_dev_preds_ridgeCV = Y_dev_preds_ridgeCV.reshape(-1, 1)\n",
    "print('RidgeCV model RMSL error:', rmsle(Y_dev, Y_dev_preds_ridgeCV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# テストデータで予測する\n",
    "\n",
    "# Ridgeモデル\n",
    "ridge_preds = ridge_model.predict(X_test)\n",
    "ridge_preds = np.expm1(ridge_preds)\n",
    "\n",
    "# RidgeCVモデル\n",
    "ridgeCV_preds = ridge_modelCV.predict(X_test)\n",
    "ridgeCV_preds = np.expm1(ridgeCV_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def aggregate_predicts3(Y1, Y2, Y3, ratio1, ratio2):\n",
    "    \"\"\"3モデルの予測値に重みを適用し、3つの予測値を1つに結合して返す\n",
    "    Y1: RNNの予測値\n",
    "    Y2: Ridgeの予測値\n",
    "    Y3: Ridge2の予測値\n",
    "    ratio1: 重み1\n",
    "    ratio2: 重み2\n",
    "    \"\"\"\n",
    "    assert Y1.shape == Y2.shape\n",
    "    return Y1*ratio1 + Y2*ratio2 + Y3*(1.0 - ratio1-ratio2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 3モデルのアンサンブル\n",
    "# 実行結果は書籍に掲載のものと一致するものではありません\n",
    "\n",
    "best1 = 0\n",
    "best2 = 0\n",
    "lowest = 0.99\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        r = i*0.01\n",
    "        r2 = j*0.01\n",
    "        # r+r2が1.0以下なら\n",
    "        if r+r2 < 1.0:\n",
    "            # 3モデルの予測値に重みを適用して新たな予測値を取得\n",
    "            Y_dev_preds = aggregate_predicts3(\n",
    "                Y_dev_preds_rnn,     # RNNの検証データ予測\n",
    "                Y_dev_preds_ridgeCV, # Ridgeの検証データ予測\n",
    "                Y_dev_preds_ridge,   # RidgeCVの検証データ予測\n",
    "                r, r2)               # 増加中の重み\n",
    "            # 現在の重みを適用後の予測値と検証データの正解値との損失を求める\n",
    "            fpred = rmsle(Y_dev, Y_dev_preds)\n",
    "            # 現在の損失が小さければ重みの比率を記録する\n",
    "            if fpred < lowest:\n",
    "                best1 = r\n",
    "                best2 = r2\n",
    "                # 現在のベストな損失として記録する\n",
    "                lowest = fpred\n",
    "            #print(str(r)+\"-RMSL error for RNN + Ridge + RidgeCV on dev set:\", fpred)\n",
    "\n",
    "# 3モデルのアンサンブルによる検証データの予測\n",
    "Y_dev_preds = aggregate_predicts3(\n",
    "    Y_dev_preds_rnn,\n",
    "    Y_dev_preds_ridgeCV,\n",
    "    Y_dev_preds_ridge, best1, best2)\n",
    "\n",
    "print('r1:', best1)\n",
    "print('r2:', best2)\n",
    "print('r3:', 1.0-best1-best2)\n",
    "print(\"(Best) RMSL error for RNN + Ridge + RidgeCV on dev set:\\n\",\n",
    "      rmsle(Y_dev, Y_dev_preds)) # Y_dev_predsのRMSLでの損失を調べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3モデルのアンサンブルで予測する\n",
    "preds = aggregate_predicts3(rnn_preds,     # RNNのテストデータの予測\n",
    "                            ridgeCV_preds, # Ridgeのテストデータの予測\n",
    "                            ridge_preds,   # RidgeCVのテストデータの予測\n",
    "                            best1, best2)  # ベストな重み\n",
    "submission = pd.DataFrame({\n",
    "        \"test_id\": test_df.test_id,\n",
    "        \"price\": preds.reshape(-1),\n",
    "})\n",
    "submission.to_csv(\"./rnn_ridge_submission_best.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_real = datetime.now()\n",
    "execution_time_real = stop_real-start_real \n",
    "print(execution_time_real)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
