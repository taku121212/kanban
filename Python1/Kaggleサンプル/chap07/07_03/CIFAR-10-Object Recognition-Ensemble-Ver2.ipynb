{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"データを用意する\n",
    "    \n",
    "    Returns:\n",
    "    X_train(ndarray):\n",
    "        訓練データ(50000.32.32.3)\n",
    "    X_test(ndarray):\n",
    "        テストデータ(10000.32.32.3)\n",
    "    y_train(ndarray):\n",
    "        訓練データのOne-Hot化した正解ラベル(50000,10)\n",
    "    y_train(ndarray):\n",
    "        テストデータのOne-Hot化した正解ラベル10000,10)\n",
    "    y_test_label(ndarray):\n",
    "        テストデータの正解ラベル(10000)\n",
    "    \"\"\"\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    # 訓練用とテスト用の画像データを標準化する\n",
    "    # 4次元テンソルのすべての軸方向に対して平均、標準偏差を求めるので\n",
    "    # axis=(0,1,2,3)は省略してもよい\n",
    "    mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "    std = np.std(X_train,axis=(0,1,2,3))\n",
    "    # 標準化する際に分母の標準偏差に極小値を加える\n",
    "    x_train = (X_train-mean)/(std+1e-7)\n",
    "    x_test = (X_test-mean)/(std+1e-7)\n",
    "    \n",
    "    # テストデータの正解ラベルを2階テンソルから1階テンソルへフラット化\n",
    "    y_test_label = np.ravel(y_test)\n",
    "    # 訓練データとテストデータの正解ラベルをOne-Hot表現に変換(10クラス化)\n",
    "    y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, y_test_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Dense, Activation\n",
    "from keras.layers import AveragePooling2D, GlobalAvgPool2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "\n",
    "\"\"\"basic_conv_block1()\n",
    "   basic_conv_block2()\n",
    "   畳み込み層を生成する\n",
    "   \n",
    "   Parameters: inp(Input): 入力層\n",
    "               fsize(int): フィルターのサイズ\n",
    "               layers(int) : 層の数\n",
    "   Returns:\n",
    "     Conv2Dを格納したTensorオブジェクト\n",
    "\"\"\"\n",
    "def basic_conv_block1(inp, fsize, layers):\n",
    "    x = inp\n",
    "    for i in range(layers):\n",
    "        x = Conv2D(\n",
    "            filters=fsize,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def basic_conv_block2(inp, fsize, layers):\n",
    "    weight_decay = 1e-4 # ハイパーパラメーターの値\n",
    "    x = inp\n",
    "    for i in range(layers):\n",
    "        x = Conv2D(\n",
    "            filters=fsize,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            kernel_regularizer=regularizers.l2(weight_decay)\n",
    "            )(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def create_cnn(model_num):\n",
    "    \"\"\"モデルを生成する\n",
    "\n",
    "    Parameters: model_num(int):\n",
    "      モデルの番号\n",
    "    Returns:\n",
    "      Conv2Dを格納したModelオブジェクト\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(32,32,3))\n",
    "    if model_num < 5:\n",
    "        x = basic_conv_block1(inp, 64, 3)\n",
    "        x = AveragePooling2D(2)(x)\n",
    "        x = basic_conv_block1(x, 128, 3)\n",
    "        x = AveragePooling2D(2)(x)\n",
    "        x = basic_conv_block1(x, 256, 3)\n",
    "        x = GlobalAvgPool2D()(x)\n",
    "        x = Dense(10, activation='softmax')(x)\n",
    "        model = Model(inp, x)\n",
    "    else:\n",
    "        x = basic_conv_block2(inp, 64, 3)\n",
    "        x = AveragePooling2D(2)(x)\n",
    "        x = basic_conv_block2(x, 128, 3)\n",
    "        x = AveragePooling2D(2)(x)\n",
    "        x = basic_conv_block2(x, 256, 3)\n",
    "        x = GlobalAvgPool2D()(x)\n",
    "        x = Dense(10, activation='softmax')(x)\n",
    "        model = Model(inp, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ensemble_average(models, X):\n",
    "    \"\"\"確率の平均をとるアンサンブル\n",
    "    \n",
    "    Parameters:\n",
    "        models(list): Modelオブジェクトのリスト\n",
    "        X(array): 検証用のデータ\n",
    "    Returns:\n",
    "        各画像の正解ラベルを格納した(,10000)のndarray\n",
    "    \"\"\"\n",
    "    # 検証結果のNumpy配列を格納する変数\n",
    "    preds_sum = None\n",
    "    # modelsから更新をフリーズされたモデルを取り出す\n",
    "    for model in models:\n",
    "        if preds_sum is None:\n",
    "            # 1番目のモデルが推定した各クラスの確率を代入\n",
    "            # preds_sumの形状は(データ数,クラス数)\n",
    "            preds_sum = model.predict(X)\n",
    "        else:\n",
    "            # 2番目のモデル以降は推定確率を各クラスごとに加算する\n",
    "            preds_sum += model.predict(X)\n",
    "    # 各クラスの推定確率の平均を(データ数,クラス数)の形状で取得\n",
    "    probs = preds_sum / len(models)\n",
    "    # 推定確率の平均(データ数,クラス数)の各行(axis=1)から\n",
    "    # 最大値のインデックスを取得して(,データ数)の形状で返す\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class Checkpoint(Callback):\n",
    "    \"\"\"Callbackのサブクラス\n",
    "    \n",
    "    Attributes:\n",
    "        model(object): Modelオブジェクト\n",
    "        filepath(str): 重みを保存するファイルのパス\n",
    "        best_val_acc\n",
    "    \"\"\"\n",
    "    def __init__(self, model, filepath):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model(Model): 現在実行中のModelオブジェクト\n",
    "            filepath(str): 重みを保存するファイルのパス\n",
    "            best_val_acc(int): 1モデルの最も高い精度を保持\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.filepath = filepath\n",
    "        self.best_val_acc = 0.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        \"\"\"エポック終了時に呼ばれるメソッドをオーバーライド\n",
    "        \n",
    "        これまでのエポックより精度が高い場合は重みをファイルに保存する\n",
    "        \n",
    "        Parameters:\n",
    "            epoch(int): エポックの回数\n",
    "            logs(dict): {'val_acc':損失, 'val_acc':精度}\n",
    "        \"\"\"\n",
    "        if self.best_val_acc < logs['val_acc']:\n",
    "            # 前回のエポックより精度が高い場合は重みを保存する\n",
    "            self.model.save_weights(self.filepath)  # ファイルパス\n",
    "            # 精度をlogsに保存\n",
    "            self.best_val_acc = logs['val_acc']\n",
    "            # 重みが保存されたことを精度と共に通知する\n",
    "            print('Weights saved.', self.best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import History\n",
    "\n",
    "def train(X_train, X_test, y_train, y_test, y_test_label):\n",
    "    \"\"\"学習を行う\n",
    "    \n",
    "    Parameters:\n",
    "        X_train(ndarray): 訓練データ\n",
    "        X_test(ndarray): 訓練データの正解ラベル\n",
    "        y_train(ndarray): テストデータ\n",
    "        y_test(ndarray): テストデータの正解ラベル(One-Hot表)\n",
    "        y_test_label(ndarray): テストデータの正解ラベル\n",
    "    \"\"\"\n",
    "    n_estimators = 9  # アンサンブルするモデルの数\n",
    "    batch_size = 1024 # ミニバッチの数\n",
    "    epoch = 80        # エポック数   \n",
    "    models = []       # モデルを格納するリスト\n",
    "    # 各モデルの学習履歴を保持するdict\n",
    "    global_hist = {\"hists\":[], \"ensemble_test\":[]}\n",
    "    # 各モデルの推測結果を登録する2階テンソルを0で初期化\n",
    "    # (データ数, モデル数)\n",
    "    single_preds = np.zeros((X_test.shape[0], # 行数は画像の枚数と同じ\n",
    "                             n_estimators))   # 列数はネットワークの数\n",
    "\n",
    "    # モデルの数だけ繰り返す\n",
    "    for i in range(n_estimators):\n",
    "        # 何番目のモデルかを表示\n",
    "        print('Model',i+1)\n",
    "        # CNNのモデルを生成,引数はモデルの番号\n",
    "        train_model = create_cnn(i)\n",
    "        # モデルをコンパイルする\n",
    "        train_model.compile(optimizer='adam',\n",
    "                            loss='categorical_crossentropy',\n",
    "                            metrics=[\"acc\"])\n",
    "        # コンパイル後のモデルをリストに追加\n",
    "        models.append(train_model)\n",
    "\n",
    "        # コールバックに登録するHistoryオブジェクトを生成\n",
    "        hist = History()\n",
    "        # コールバックに登録するCheckpointオブジェクトを生成\n",
    "        cp = Checkpoint(train_model,         # Modelオブジェクト\n",
    "                        f'weights_{i}.h5') # 重みを保存するファイル名\n",
    "        # ステップ減衰関数\n",
    "        def step_decay(epoch):\n",
    "            initial_lrate = 0.001 # ベースにする学習率\n",
    "            drop = 0.5            # 減衰率\n",
    "            epochs_drop = 10.0     # ステップ減衰は10エポックごと\n",
    "            lrate = initial_lrate * math.pow(\n",
    "                drop,\n",
    "                math.floor((1+epoch)/epochs_drop)\n",
    "            )\n",
    "            return lrate\n",
    "            \n",
    "        lrate = LearningRateScheduler(step_decay) # スケジューラ―オブジェクト\n",
    "        \n",
    "        # データ拡張\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=15,      # 15度の範囲でランダムに回転させる\n",
    "            width_shift_range=0.1,  # 横サイズの0.1の割合でランダムに水平移動\n",
    "            height_shift_range=0.1, # 縦サイズの0.1の割合でランダムに垂直移動\n",
    "            horizontal_flip=True,   # 水平方向にランダムに反転、左右の入れ替え\n",
    "            zoom_range=0.2,         # ランダムに拡大\n",
    "            )\n",
    "\n",
    "        # 学習を行う\n",
    "        train_model.fit_generator(\n",
    "            datagen.flow(X_train,\n",
    "                         y_train,\n",
    "                         batch_size=batch_size),\n",
    "            #batch_size=batch_size,\n",
    "            epochs=epoch,\n",
    "            steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=1,\n",
    "            callbacks=[hist, cp, lrate] # コールバック\n",
    "            )       \n",
    "\n",
    "        # 学習に用いたモデルで最も精度が高かったときの重みを読み込む\n",
    "        train_model.load_weights(f'weights_{i}.h5')\n",
    "        \n",
    "        # 対象のモデルのすべての重み更新をフリーズする\n",
    "        for layer in train_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # テストデータで推測し、各画像ごとにラベルの最大値を求め、\n",
    "        # 対象のインデックスを正解ラベルとしてsingle_predsのi列に格納\n",
    "        single_preds[:, i] = np.argmax(train_model.predict(X_test),\n",
    "                                       axis=-1) # 行ごとの最大値を求める\n",
    "\n",
    "        # 学習に用いたモデルの学習履歴をglobal_histのhistsキーに登録\n",
    "        global_hist['hists'].append(hist.history)\n",
    "        \n",
    "        # 平均をとるアンサンブルを実行\n",
    "        ensemble_test_pred = ensemble_average(models, X_test)\n",
    "        \n",
    "        # scikit-learn.accuracy_score()でアンサンブルによる精度を取得\n",
    "        ensemble_test_acc = accuracy_score(y_test_label, ensemble_test_pred)\n",
    "        \n",
    "        # アンサンブルの精度をglobal_histのensemble_testキーに追加\n",
    "        global_hist['ensemble_test'].append(ensemble_test_acc)\n",
    "        # 現在のアンサンブルの精度を出力\n",
    "        print('Current Ensemble Test Accuracy : ', ensemble_test_acc)\n",
    "\n",
    "    global_hist['corrcoef'] = np.corrcoef(single_preds,\n",
    "                                          rowvar=False) # 列ごとの相関係数を求める\n",
    "    print('Correlation predicted value')\n",
    "    print(global_hist['corrcoef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行部\n",
    "\n",
    "# データを用意する\n",
    "X_train, X_test, y_train, y_test, y_test_label  = prepare_data()\n",
    "\n",
    "# アンサンブルを実行\n",
    "train(X_train, X_test, y_train, y_test, y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
