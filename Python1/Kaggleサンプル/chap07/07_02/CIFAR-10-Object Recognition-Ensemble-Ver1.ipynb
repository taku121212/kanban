{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## このプログラムは「CIFAR-10 - Object Recognition in Images」において\n",
    "## 作成したノートブックで動作します\n",
    "## ローカル環境のJupyter Notebookで作成したノートブックでも動作可能です\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"データを用意する\n",
    "    \n",
    "    Returns:\n",
    "    X_train(ndarray):\n",
    "        訓練データ(50000.32.32.3)\n",
    "    X_test(ndarray):\n",
    "        テストデータ(10000.32.32.3)\n",
    "    y_train(ndarray):\n",
    "        訓練データのOne-Hot化した正解ラベル(50000,10)\n",
    "    y_train(ndarray):\n",
    "        テストデータのOne-Hot化した正解ラベル10000,10)\n",
    "    y_test_label(ndarray):\n",
    "        テストデータの正解ラベル(10000)\n",
    "    \"\"\"\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    # 訓練用とテスト用の画像データを標準化する\n",
    "    # 4次元テンソルのすべての軸方向に対して平均、標準偏差を求めるので\n",
    "    # axis=(0,1,2,3)は省略してもよい\n",
    "    mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "    std = np.std(X_train,axis=(0,1,2,3))\n",
    "    # 標準化する際に分母の標準偏差に極小値を加える\n",
    "    x_train = (X_train-mean)/(std+1e-7)\n",
    "    x_test = (X_test-mean)/(std+1e-7)\n",
    "    \n",
    "    # テストデータの正解ラベルを2階テンソルから1階テンソルへフラット化\n",
    "    y_test_label = np.ravel(y_test)\n",
    "    # 訓練データとテストデータの正解ラベルをOne-Hot表現に変換(10クラス化)\n",
    "    y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, y_test_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Dense, Activation\n",
    "from keras.layers import AveragePooling2D, GlobalAvgPool2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "\n",
    "def make_convlayer(input, fsize, layers):\n",
    "    \"\"\"畳み込み層を生成する\n",
    "\n",
    "       Parameters: inp(Input): 入力層\n",
    "                   fsize(int): フィルターのサイズ\n",
    "                   layers(int) : 層の数\n",
    "       Returns:\n",
    "         Conv2Dを格納したTensorオブジェクト\n",
    "    \"\"\"\n",
    "    x = input\n",
    "    for i in range(layers):\n",
    "        x =Conv2D(\n",
    "            filters=fsize,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"モデルを生成する\n",
    "\n",
    "    Returns:\n",
    "      Conv2Dを格納したModelオブジェクト\n",
    "    \"\"\"\n",
    "    input = Input(shape=(32,32,3))\n",
    "    x = make_convlayer(input, 64, 3)\n",
    "    x = AveragePooling2D(2)(x)\n",
    "    x = make_convlayer(x, 128, 3)\n",
    "    x = AveragePooling2D(2)(x)\n",
    "    x = make_convlayer(x, 256, 3)\n",
    "    x = GlobalAvgPool2D()(x)\n",
    "    x = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(input, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def ensemble_majority(models, X):\n",
    "    \"\"\"多数決をとるアンサンブル\n",
    "\n",
    "    Parameters:\n",
    "        models(list): Modelオブジェクトのリスト\n",
    "        X(array): 検証用のデータ\n",
    "    Returns:\n",
    "        各画像の正解ラベルを格納した(10000)のnp.ndarray\n",
    "    \"\"\"\n",
    "    # (データ数,モデル数)のゼロ行列を作成\n",
    "    pred_labels = np.zeros((X.shape[0],   # 行数は画像の枚数と同じ\n",
    "                            len(models))) # 列数はモデルの数\n",
    "    # modelsからインデックス値と更新をフリーズされたモデルを取り出す\n",
    "    for i, model in enumerate(models):\n",
    "        # モデルごとの予測確率(データ数,クラス数)の各行(axis=1)から\n",
    "        # 最大値のインデックスをとって、(データ数,モデル数)の\n",
    "        # モデル列の各行にデータの数だけ格納する\n",
    "        pred_labels[:, i] = np.argmax(model.predict(X), axis=1)\n",
    "    # mode()でpred_labelsの各行の最頻値のみを[0]指定で取得する\n",
    "    # (データ数,1)の形状をravel()で(,データ数)の形状にフラット化する    \n",
    "    return np.ravel(mode(pred_labels, axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class Checkpoint(Callback):\n",
    "    \"\"\"Callbackのサブクラス\n",
    "    \n",
    "    Attributes:\n",
    "        model(object): 学習中のModelオブジェクト\n",
    "        filepath(str): 重みを保存するファイルのパス\n",
    "        best_val_acc : 最高精度を保持する\n",
    "    \"\"\"\n",
    "    def __init__(self, model, filepath):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model(Model): 現在実行中のModelオブジェクト\n",
    "            filepath(str): 重みを保存するファイルのパス\n",
    "            best_val_acc(int): 1モデルの最も高い精度を保持\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.filepath = filepath\n",
    "        self.best_val_acc = 0.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        \"\"\"エポック終了時に呼ばれるメソッドをオーバーライド\n",
    "        \n",
    "        これまでのエポックより精度が高い場合は重みをファイルに保存する\n",
    "        \n",
    "        Parameters:\n",
    "            epoch(int): エポックの回数\n",
    "            logs(dict): {'val_acc':損失, 'val_acc':精度}\n",
    "        \"\"\"\n",
    "        if self.best_val_acc < logs['val_acc']:\n",
    "            # 前回のエポックより精度が高い場合は重みを保存する\n",
    "            self.model.save_weights(self.filepath)  # ファイルパス\n",
    "            # 精度をlogsに保存\n",
    "            self.best_val_acc = logs['val_acc']\n",
    "            # 重みが保存されたことを精度と共に通知する\n",
    "            print('Weights saved.', self.best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import History\n",
    "\n",
    "def train(X_train, X_test, y_train, y_test, y_test_label):\n",
    "    \"\"\"\n",
    "    学習を行う\n",
    "    \n",
    "    Parameters:\n",
    "        X_train(ndarray): 訓練データ\n",
    "        X_test(ndarray): 訓練データの正解ラベル\n",
    "        y_train(ndarray): テストデータ\n",
    "        y_test(ndarray): テストデータの正解ラベル(One-Hot表)\n",
    "        y_test_label(ndarray): テストデータの正解ラベル\n",
    "    \"\"\"\n",
    "    models_num  = 5   # アンサンブルするモデルの数\n",
    "    batch_size = 1024 # ミニバッチの数\n",
    "    epoch = 80        # エポック数   \n",
    "    models = []       # モデルを格納するリスト\n",
    "    # 各モデルの学習履歴を保持するdict\n",
    "    history_all = {\"hists\":[], \"ensemble_test\":[]}\n",
    "    # 各モデルの推測結果を登録する2階テンソルを0で初期化\n",
    "    # (データ数, モデル数)\n",
    "    model_predict = np.zeros((X_test.shape[0], # 行数は画像の枚数\n",
    "                             models_num))      # 列数はモデルの数\n",
    "\n",
    "    # モデルの数だけ繰り返す\n",
    "    for i in range(models_num):\n",
    "        # 何番目のモデルかを表示\n",
    "        print('Model',i+1)\n",
    "        # CNNのモデルを生成\n",
    "        train_model = create_model()\n",
    "        # モデルをコンパイルする\n",
    "        train_model.compile(optimizer='adam',\n",
    "                            loss='categorical_crossentropy',\n",
    "                            metrics=[\"acc\"])\n",
    "        # コンパイル後のモデルをリストに追加\n",
    "        models.append(train_model)\n",
    "\n",
    "        # コールバックに登録するHistoryオブジェクトを生成\n",
    "        hist = History()\n",
    "        # コールバックに登録するCheckpointオブジェクトを生成\n",
    "        cpont = Checkpoint(train_model,       # Modelオブジェクト\n",
    "                           f'weights_{i}.h5') # 重みを保存するファイル名\n",
    "        # ステップ減衰関数\n",
    "        def step_decay(epoch):\n",
    "            initial_lrate = 0.001 # ベースにする学習率\n",
    "            drop = 0.5            # 減衰率\n",
    "            epochs_drop = 10.0    # ステップ減衰は10エポックごと\n",
    "            lrate = initial_lrate * math.pow(\n",
    "                drop,\n",
    "                math.floor((1+epoch)/epochs_drop)\n",
    "            )\n",
    "            return lrate\n",
    "            \n",
    "        lrate = LearningRateScheduler(step_decay) # スケジューラ―オブジェクト\n",
    "        \n",
    "        # データ拡張\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=15,      # 15度の範囲でランダムに回転させる\n",
    "            width_shift_range=0.1,  # 横サイズの0.1の割合でランダムに水平移動\n",
    "            height_shift_range=0.1, # 縦サイズの0.1の割合でランダムに垂直移動\n",
    "            horizontal_flip=True,   # 水平方向にランダムに反転、左右の入れ替え\n",
    "            zoom_range=0.2,         # ランダムに拡大\n",
    "            )\n",
    "\n",
    "        # 学習を行う\n",
    "        train_model.fit_generator(\n",
    "            datagen.flow(X_train,\n",
    "                         y_train,\n",
    "                         batch_size=batch_size),\n",
    "            epochs=epoch,\n",
    "            steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=1,\n",
    "            callbacks=[hist, cpont, lrate] # コールバック\n",
    "            )       \n",
    "\n",
    "        # 学習に用いたモデルで最も精度が高かったときの重みを読み込む\n",
    "        train_model.load_weights(f'weights_{i}.h5')\n",
    "        \n",
    "        # 対象のモデルのすべての重み更新をフリーズする\n",
    "        for layer in train_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # テストデータで推測し、各画像ごとにラベルの最大値を求め、\n",
    "        # 対象のインデックスを正解ラベルとしてmodel_predictのi列に格納\n",
    "        model_predict[:, i] = np.argmax(train_model.predict(X_test),\n",
    "                                       axis=-1) # 行ごとの最大値を求める\n",
    "\n",
    "        # 学習に用いたモデルの学習履歴をhistory_allのhistsキーに登録\n",
    "        history_all['hists'].append(hist.history)\n",
    "        \n",
    "        # 多数決のアンサンブルを実行\n",
    "        ensemble_test_pred = ensemble_majority(models, X_test)\n",
    "        \n",
    "        # scikit-learn.accuracy_score()でアンサンブルによる精度を取得\n",
    "        ensemble_test_acc = accuracy_score(y_test_label, ensemble_test_pred)\n",
    "        \n",
    "        # アンサンブルの精度をhistory_allのensemble_testキーに追加\n",
    "        history_all['ensemble_test'].append(ensemble_test_acc)\n",
    "        # 現在のアンサンブルの精度を出力\n",
    "        print('Current Ensemble Accuracy : ', ensemble_test_acc)\n",
    "\n",
    "    history_all['corrcoef'] = np.corrcoef(model_predict,\n",
    "                                          rowvar=False) # 列ごとの相関係数を求める\n",
    "    print('Correlation predicted value')\n",
    "    print(history_all['corrcoef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行部\n",
    "\n",
    "# データを用意する\n",
    "X_train, X_test, y_train, y_test, y_test_label  = prepare_data()\n",
    "\n",
    "# アンサンブルを実行\n",
    "train(X_train, X_test, y_train, y_test, y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
