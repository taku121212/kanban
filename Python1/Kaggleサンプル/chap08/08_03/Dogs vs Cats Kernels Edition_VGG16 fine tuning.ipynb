{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## このプログラムは「Dogs vs. Cats Redux: Kernels Edition」において\n",
    "## 作成したノートブックで動作します\n",
    "\n",
    "import pandas as pd\n",
    "import os, zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepareData():\n",
    "    \"\"\"データを読み込んで前処理を行う\n",
    "    \n",
    "    Returns:\n",
    "      train_df(Dataframe)   : 前処理後の訓練データ\n",
    "      validate_df(Dataframe): 前処理後の検証データ\n",
    "    \"\"\"\n",
    "    # 訓練データとテストデータの解凍\n",
    "    # 解凍するzipファイル名\n",
    "    data = ['train', 'test']\n",
    "\n",
    "    # train.zip、test.zipをカレントディレクトリに展開\n",
    "    for el in data:\n",
    "        with zipfile.ZipFile('../input/dogs-vs-cats-redux-kernels-edition/' + el + \".zip\", \"r\") as z:\n",
    "            z.extractall(\".\")  # extract zip files to current dir\n",
    "\n",
    "    # 訓練データのファイル名のdog.x.jpg、cat.x.jpgを使って1と0のラベルを生成\n",
    "    # trainフォルダー内のファイル名を取得してfilenamesに格納\n",
    "    filenames = os.listdir(\"./train\")\n",
    "    categories = []\n",
    "    \n",
    "    for filename in filenames:\n",
    "        # ファイル名を分割して先頭要素(dog/cat)のみを取り出し、\n",
    "        # dogは1、catは0をラベルにしてcategoryに格納\n",
    "        category = filename.split('.')[0]\n",
    "        if category == 'dog':\n",
    "        # dogならラベル1として追加\n",
    "            categories.append(1)\n",
    "        else:\n",
    "        # catならラベル0として追加\n",
    "            categories.append(0)\n",
    "\n",
    "    # dfの列filenameにファイル名filenamesを格納\n",
    "    # 列categoriesにラベルの値categoryを格納\n",
    "    df = pd.DataFrame({\n",
    "        'filename': filenames,\n",
    "        'category': categories\n",
    "    })\n",
    "    \n",
    "    # 訓練データを訓練用と検証用に分ける\n",
    "    # 訓練データの総数25000の10%を検証データにする\n",
    "    train_df, validate_df = train_test_split(df, test_size=0.1)\n",
    "    # 行インデックスを振り直す\n",
    "    train_df = train_df.reset_index()\n",
    "    validate_df = validate_df.reset_index()\n",
    "       \n",
    "    return train_df, validate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def ImageDataGenerate(train_df, validate_df):\n",
    "    \"\"\"画像を加工処理する\n",
    "    \n",
    "    Returns:\n",
    "      train_generator(DirectoryIterator):\n",
    "          加工処理後の訓練データ\n",
    "      validation_generator(DirectoryIterator):\n",
    "          加工処理後の検証データ\n",
    "    \"\"\"\n",
    "    # 画像をリサイズするサイズ\n",
    "    img_width, img_height = 224, 224\n",
    "    target_size = (img_width, img_height)\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = 16\n",
    "    \n",
    "    # ファイル名の列名とラベルの列名\n",
    "    x_col, y_col = 'filename', 'category'\n",
    "    # flow_from_dataframe()で画像を生成する際のclass_modeオプションの値\n",
    "    # ジェネレーターが返すラベルの配列の形状として'binary'を格納\n",
    "    class_mode = 'binary'\n",
    "\n",
    "    # 訓練データを加工するジェネレーターを生成\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1\n",
    "    )\n",
    "\n",
    "    # flow_from_dataframe()の引数class_mode = \"binary\"の場合、\n",
    "    # ラベルが格納されたtrain_dfのy_col = 'category'の列の値は\n",
    "    # 文字列であることが必要なので、1と0の数値を文字列に変換しておく\n",
    "    train_df['category'] = train_df['category'].astype(str)  #optional\n",
    "\n",
    "    # ジェネレータを使って訓練データを生成\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        train_df,     # 訓練用のデータフレーム\n",
    "        \"./train/\",   # 画像データのディレクトリ\n",
    "        x_col=x_col,  # ファイル名が格納された列\n",
    "        y_col=y_col,  # ラベルが格納された列 (文字列に変換済み)\n",
    "        class_mode=class_mode,   # ラベルの配列の形状\n",
    "        target_size=target_size, # 画像のサイズ\n",
    "        batch_size=batch_size    # ミニバッチのサイズ\n",
    "    )\n",
    "\n",
    "    # 検証データを加工するジェネレーター\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # flow_from_dataframe()の引数class_mode = \"binary\"の場合、\n",
    "    # ラベルが格納されたvalidate_dfのy_col = 'category'の列の値は\n",
    "    # 文字列であることが必要なので、1と0の数値を文字列に変換しておく\n",
    "    validate_df['category'] = validate_df['category'].astype(str)\n",
    "\n",
    "    # ジェネレータを使って検証データを生成\n",
    "    validation_generator = validation_datagen.flow_from_dataframe(\n",
    "        validate_df,   # 検証用のデータフレーム\n",
    "        \"./train/\",    # 画像データのディレクトリ\n",
    "        x_col=x_col,   # ファイル名が格納された列\n",
    "        y_col=y_col,   # ラベルが格納された列(文字列に変換済み)\n",
    "        class_mode=class_mode,   # ラベルの配列の形状\n",
    "        target_size=target_size, # 画像のサイズ\n",
    "        batch_size=batch_size    # ミニバッチのサイズ\n",
    "    )\n",
    "    \n",
    "    # 生成した訓練データと検証データを返す\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalMaxPooling2D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "def train_FClayer(train_generator, validation_generator):\n",
    "    \"\"\"ファインチューニングしたVGG16で学習する\n",
    "    \n",
    "    Returns:\n",
    "      history(Historyオブジェクト)\n",
    "    \"\"\"\n",
    "    # 画像のサイズを取得\n",
    "    image_size = len(train_generator[0][0][0])\n",
    "    # 入力データの形状をタプルにする\n",
    "    input_shape = (image_size, image_size, 3)\n",
    "    # ミニバッチのサイズを取得\n",
    "    batch_size = len(train_generator[0][0])\n",
    "    # 訓練データの数を取得(バッチの数×ミニバッチサイズ)\n",
    "    total_train = len(train_generator)*batch_size\n",
    "    # 検証データの数を取得(バッチの数×ミニバッチサイズ)\n",
    "    total_validate = len(validation_generator)*batch_size\n",
    "\n",
    "    # VGG16モデルを学習済みの重みと共に読み込む\n",
    "    pre_trained_model = VGG16(\n",
    "        include_top=False,            # 全結合層（FC）は読み込まない\n",
    "        weights='imagenet',           # ImageNetで学習した重みを利用\n",
    "        input_shape=input_shape   # 入力データの形状\n",
    "    )\n",
    "\n",
    "    for layer in pre_trained_model.layers[:15]:\n",
    "        # 第1～第15層までの重みを凍結\n",
    "        layer.trainable = False\n",
    "\n",
    "    for layer in pre_trained_model.layers[15:]:\n",
    "        # 第16層以降の重みを更新可能にする\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Sequentualオブジェクトを生成\n",
    "    model = Sequential()\n",
    "\n",
    "    # VGG16モデルを追加\n",
    "    model.add(pre_trained_model)\n",
    "\n",
    "    # (batch_size, rows, cols, channels)の4階テンソルに\n",
    "    # プーリング演算適用後、(batch_size, channels)の2階テンソルにフラット化\n",
    "    model.add(\n",
    "        GlobalMaxPooling2D())\n",
    "\n",
    "    # 全結合層\n",
    "    model.add(\n",
    "        Dense(512,               # ユニット数512\n",
    "              activation='relu') # 活性化関数はReLU\n",
    "    )\n",
    "    # 50%のドロップアウト\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # 出力層\n",
    "    model.add(\n",
    "        Dense(1,                    # ユニット数1\n",
    "              activation='sigmoid') # 活性化関数はSigmoid\n",
    "    )\n",
    "    \n",
    "    # モデルのコンパイル\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # コンパイル後のサマリを表示\n",
    "    model.summary()\n",
    "\n",
    "    # 学習率をスケジューリングする\n",
    "    def step_decay(epoch):\n",
    "        initial_lrate = 0.00001 # 学習率の初期値\n",
    "        drop = 0.5              # 減衰率は50%\n",
    "        epochs_drop = 10.0      # 10エポック毎に減衰する\n",
    "        lrate = initial_lrate * math.pow(\n",
    "            drop,\n",
    "            math.floor((epoch)/epochs_drop)\n",
    "        )\n",
    "        return lrate\n",
    "\n",
    "    # 学習率のコールバック\n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "    # ファインチューニングモデルで学習する\n",
    "    epochs = 40   # エポック数\n",
    "    history = model.fit(\n",
    "        # 訓練データ\n",
    "        train_generator,\n",
    "        # エポック数\n",
    "        epochs=epochs,\n",
    "        # 訓練時のステップ数\n",
    "        validation_data=validation_generator,\n",
    "        # 検証データ\n",
    "        validation_steps=total_validate//batch_size,\n",
    "        # 検証時のステップ数\n",
    "        steps_per_epoch=total_train//batch_size,\n",
    "        # 学習の進捗状況を出力する    \n",
    "        verbose=1,\n",
    "        # 学習率のスケジューラーをコール\n",
    "        callbacks=[lrate]\n",
    "    )\n",
    "    \n",
    "    # historyを返す\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理したデータを取得\n",
    "train_df, validate_df = prepareData()\n",
    "# ジェネレーターで滑降する\n",
    "train_generator, validation_generator = ImageDataGenerate(train_df, validate_df)\n",
    "# VGG16の出力をFCネットワークで学習\n",
    "history = train_FClayer(train_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_acc_loss(history):\n",
    "    # 精度の推移をプロット\n",
    "    plt.plot(history.history['accuracy'],\"-\",label=\"accuracy\")\n",
    "    plt.plot(history.history['val_accuracy'],\"-\",label=\"val_acc\")\n",
    "    plt.title('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # 損失の推移をプロット\n",
    "    plt.plot(history.history['loss'],\"-\",label=\"loss\",)\n",
    "    plt.plot(history.history['val_loss'],\"-\",label=\"val_loss\")\n",
    "    plt.title('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "# 損失と精度をグラフに出力\n",
    "plot_acc_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
